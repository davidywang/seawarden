{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'../targets_2_simrdwn/G/predictions_G.csv' does not exist: b'../targets_2_simrdwn/G/predictions_G.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-4228c2c8480f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcountries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m     \u001b[0msimrdwn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-4228c2c8480f>\u001b[0m in \u001b[0;36msimrdwn\u001b[1;34m(n)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'../targets_2_simrdwn/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'predictions_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mdetections\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'country:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'total SIMRDWN predictions'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\thesis\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\thesis\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\thesis\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\thesis\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\thesis\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'../targets_2_simrdwn/G/predictions_G.csv' does not exist: b'../targets_2_simrdwn/G/predictions_G.csv'"
     ]
    }
   ],
   "source": [
    "#import os\n",
    "import math\n",
    "import fiona \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from geopandas.tools import sjoin\n",
    "from shapely.geometry import Polygon, Point, mapping\n",
    "from explode import explode\n",
    "from coord import coord\n",
    "\n",
    "wgs84= {'init' :'EPSG:4326'}\n",
    "\n",
    "#SET COUNTRY/CRS <-------------------------------------------------------------------------------CHANGE\n",
    "countries = ['TUR', 'EPSG:5637'],['ESP', 'EPSG:2062'],['HRV', 'EPSG:3765'],['ITA', 'EPSG:7794'],['CYP', 'EPSG:6312'],['MLT', 'EPSG:3034'],['FRA', 'EPSG:2154'],['ALB', 'EPSG:6962'] \n",
    "\n",
    "def simrdwn(n):\n",
    "    #open file\n",
    "    f = '../targets_2_simrdwn/' + n[0] + '/'\n",
    "    file = f + 'predictions_' + n[0] + '.csv'\n",
    "    detections = pd.read_csv(file, delimiter=',', header=0)\n",
    "    print('country:', n[0])\n",
    "    print(len(detections), 'total SIMRDWN predictions')\n",
    "\n",
    "    #georeference prediction bounding boxes and convert to centroid\n",
    "    selected = detections\n",
    "\n",
    "    lon_list, lat_list, radius_list, diameter_list = [], [], [], []\n",
    "    for i in range(len(selected)):\n",
    "        img = detections['img_file'].iloc[i]\n",
    "\n",
    "    #extract bottom right coordinate from image name\n",
    "        br_lon = float(img.split('_')[4] + '.' + img.split('_')[5]) #lon = x                                       \n",
    "        br_lat = float(img.split('_')[2] + '.' + img.split('_')[3]) #lat = y\n",
    "\n",
    "        #extract top left coordinate from image name\n",
    "        tl_lon = float(img.split('_')[8] + '.' + (img.split('_')[9]).split('.')[0]) #lon = x\n",
    "        tl_lat = float(img.split('_')[6] + '.' + img.split('_')[7]) #lat = y    \n",
    "\n",
    "    #     #FOR GREECE ONLY - extract bottom right coordinate from image name\n",
    "    #     br_lon = float(img.split('_')[8] + '.' + img.split('_')[9]) #lon = x                                       \n",
    "    #     br_lat = float(img.split('_')[6] + '.' + img.split('_')[7]) #lat = y\n",
    "\n",
    "    #     #extract top left coordinate from image name\n",
    "    #     tl_lon = float(img.split('_')[12] + '.' + (img.split('_')[13]).split('.')[0]) #lon = x\n",
    "    #     tl_lat = float(img.split('_')[10] + '.' + img.split('_')[11]) #lat = y\n",
    "\n",
    "        #image dimensions\n",
    "        img_w = detections['img_width'].iloc[i]\n",
    "        img_h = detections['img_height'].iloc[i]\n",
    "        width = abs(br_lon - tl_lon) \n",
    "        height = abs(tl_lat - br_lat)\n",
    "\n",
    "        #decimal degrees per pixel\n",
    "        res_x = width / img_w #image width\n",
    "        res_y = height / img_h #image height\n",
    "\n",
    "        #res\n",
    "        lat = tl_lat * math.pi / 180\n",
    "        res = 156543.04 * math.cos(lat) / (2 ** 18)\n",
    "\n",
    "        #convert bounding box to centroid\n",
    "        xmin = detections['xmin'].iloc[i]\n",
    "        ymin = detections['ymin'].iloc[i]\n",
    "        xmax = detections['xmax'].iloc[i]\n",
    "        ymax = detections['ymax'].iloc[i]\n",
    "        x = (xmin + xmax) / 2\n",
    "        y = (ymin + ymax) / 2\n",
    "\n",
    "        #convert centroid point to lat/lon   \n",
    "        x_center = tl_lon + (x * res_x) \n",
    "        y_center = tl_lat - (y * res_y) \n",
    "\n",
    "        #estimate radius of detection\n",
    "        w = xmax-xmin \n",
    "        h = ymax-ymin \n",
    "        radius = (((w+h)/2)/4)*1.1\n",
    "        diameter = radius * 2\n",
    "        lon_list.append(x_center)\n",
    "        lat_list.append(y_center)\n",
    "        radius_list.append(radius)\n",
    "        diameter_list.append(diameter)\n",
    "\n",
    "    #add attributes to df\n",
    "    d2 = selected.reset_index(drop=True) #super important\n",
    "    d2 = d2.drop(columns=['label'])\n",
    "    d2['x']=lon_list\n",
    "    d2['y']=lat_list\n",
    "    d2['radius']=np.round(radius_list,2)\n",
    "    d2['diameter']=np.round(diameter_list,2)\n",
    "    print(len(d2), 'total SIMRDWN predictions')\n",
    "\n",
    "    #add geometry information to df \n",
    "    geometry = [Point(i) for i in zip(lon_list, lat_list)]\n",
    "    d2 = gpd.GeoDataFrame(d2, geometry=geometry, crs=wgs84)\n",
    "    d2 = d2.to_crs({'init': n[1]})\n",
    "\n",
    "    #select predictions within search area\n",
    "    # search_area = gpd.read_file('../0_search_areas/4_search_area/search_area_100m_' + n + '.shp')\n",
    "    # search_area = search_area.to_crs({'init': n[1]})\n",
    "\n",
    "    # d2 = sjoin(d2, search_area, how='inner', op='within')\n",
    "    # print(len(d2), 'predictions within search area')\n",
    "\n",
    "    #select predictions by diameter\n",
    "    d = 55\n",
    "    d2 = d2[d2['diameter'] <= d]\n",
    "    geometry = [Point(i) for i in zip(d2['x'], d2['y'])]\n",
    "\n",
    "    d2 = d2[['radius', 'geometry']]\n",
    "    print(len(d2), 'total SIMRDWN predictions less than or equal to', d, 'meters')\n",
    "\n",
    "    #aggregate predictions \n",
    "    buffer = gpd.GeoDataFrame(geometry = d2.buffer(10)) #buffer by 10 meters\n",
    "    buffer['Dissolve'] = 0\n",
    "    buffer_dis = buffer.dissolve(by='Dissolve')\n",
    "    buffer_exploded = explode(buffer_dis)    \n",
    "    print(len(d2), 'predictions aggregated to', len(buffer_exploded), 'predictions')\n",
    "\n",
    "    #calculate mean radius of aggregated predictions\n",
    "    d2_ag = gpd.sjoin(d2, buffer_exploded, how=\"inner\", op='intersects')\n",
    "    centroids = gpd.GeoDataFrame(geometry = buffer_exploded.centroid, crs=wgs84)\n",
    "    centroids['radius']=d2_ag.groupby('index_right')['radius'].mean()\n",
    "    centroids.crs={'init' : n[1]}\n",
    "    print(len(centroids), 'predictions')\n",
    "\n",
    "    #generate farmsite polygons\n",
    "    b = 100\n",
    "    buffer = gpd.GeoDataFrame(geometry = centroids.buffer(b)) #buffer by 100m\n",
    "    buffer['Dissolve'] = 0\n",
    "    buffer_dis = buffer.dissolve(by='Dissolve')\n",
    "    buffer_exploded = explode(buffer_dis)    \n",
    "    buffer_exploded.crs={'init' : n[1]}\n",
    "\n",
    "    #add number of predictions per farmsite to farmsite polygons\n",
    "    count=sjoin(centroids, buffer_exploded, how='inner', op='within')\n",
    "    buffer_exploded['cage count']=count.groupby('index_right')['index_right'].count()\n",
    "    buffer_exploded['cage count']=count.groupby('index_right')['index_right'].count()\n",
    "    buffer_exploded[\"farm ID\"] = buffer_exploded.index + 1 #add farm ID\n",
    "\n",
    "    print(len(buffer_exploded), 'total farmsite predictions based on farmsites')\n",
    "\n",
    "    #select farmsites by number of predictions\n",
    "    p = 4 #number of predictions per farmsite\n",
    "    farmsites = buffer_exploded[buffer_exploded['cage count'] >= p]\n",
    "    print(len(farmsites), 'farm site predictions containing', p, 'or more net pen predictions')\n",
    "\n",
    "    #select predictions by farmsites\n",
    "    select = farmsites\n",
    "    select['predictions'] = 0\n",
    "    select = select.dissolve(by='predictions')\n",
    "    mask = centroids.within(select.loc[0, 'geometry'])\n",
    "    farmsites=farmsites.drop(['predictions'], axis=1)\n",
    "    d2_clip = centroids.loc[mask]\n",
    "\n",
    "    print(len(d2_clip), 'net pen predictions within farm sites')\n",
    "    print(d2_clip.crs)\n",
    "\n",
    "    #add id and farm_id to predictions\n",
    "    d2_clip_id=sjoin(farmsites, d2_clip, how='right', op='intersects')\n",
    "    d2_clip_id=d2_clip_id.reset_index(drop=True)\n",
    "    d2_clip_id[\"cage ID\"] = d2_clip_id.index + 1 \n",
    "    d2_clip_id['diameter']=d2_clip_id['radius'] * 2\n",
    "\n",
    "    d2_clip_id['radius']=round(d2_clip_id['radius'],2)\n",
    "    d2_clip_id['diameter']=round(d2_clip_id['diameter'],2)\n",
    "\n",
    "    d2_clip=d2_clip_id[['farm ID', 'cage ID', 'radius', 'diameter', 'geometry']]\n",
    "    print(len(d2_clip), 'net pen predictions')\n",
    "\n",
    "    #generate farmsite extents\n",
    "    envelope = gpd.GeoDataFrame(geometry = farmsites.envelope)\n",
    "    farmsites2=farmsites.copy()\n",
    "    farmsites2['geometry']=envelope['geometry']\n",
    "    farmsites2=farmsites2[['farm ID', 'cage count', 'geometry']]\n",
    "    print(len(farmsites2), 'farm site predictions')\n",
    "\n",
    "    #exclusion of false positives\n",
    "    path= f + n[0] + '_exclude.kml'\n",
    "    gpd.io.file.fiona.drvsupport.supported_drivers['KML'] = 'rw'\n",
    "    exclude = gpd.read_file(path, driver='KML')\n",
    "    exclude = exclude.to_crs({'init': n[1]})\n",
    "\n",
    "    #create exclusion mask\n",
    "    exclude['Dissolve'] = 0\n",
    "    exclude_dis = exclude.dissolve(by='Dissolve')\n",
    "\n",
    "    #eliminate net pen false positives\n",
    "    mask = ~d2_clip.within(exclude_dis.loc[0, 'geometry'])\n",
    "    d2_clip = d2_clip.loc[mask]\n",
    "    print('final number of predicted net pens:', len(d2_clip))\n",
    "\n",
    "    #eliminate farmsite false positives\n",
    "    mask2 = ~farmsites2.within(exclude_dis.loc[0, 'geometry'])\n",
    "    farmsites2 = farmsites2.loc[mask2]\n",
    "    print('final number of predicted farm sites:', len(farmsites2))\n",
    "\n",
    "    # #generate net pen buffers\n",
    "    def buffer(row):\n",
    "         return row.geometry.buffer(row.radius)   \n",
    "\n",
    "    d2_clip2 = d2_clip.copy()\n",
    "    d2_clip2.crs\n",
    "\n",
    "    buff = d2_clip2['geometry'] = d2_clip2.apply(buffer, axis=1)\n",
    "    circles = gpd.GeoDataFrame(d2_clip2, geometry = buff, crs={'init': n[1]})\n",
    "\n",
    "    #generate farmsite centroids\n",
    "    farmsites3 = farmsites2.copy()\n",
    "    farm_pts = gpd.GeoDataFrame(farmsites3, geometry = farmsites3.centroid)\n",
    "    \n",
    "    #generate buffer and envelope for bing aquistion\n",
    "    centroid_buffer = gpd.GeoDataFrame(geometry = farm_pts.buffer(500))\n",
    "    envelope = gpd.GeoDataFrame(farmsites3, geometry = centroid_buffer.envelope)\n",
    "    envelope = envelope['geometry'].to_crs(epsg=4326)\n",
    "    #extract lat/long for each square polygon (envelope)\n",
    "    coord_list = []\n",
    "    for i in envelope.index:\n",
    "        coords = mapping(envelope.geometry[i])['coordinates']\n",
    "        coord_list.append(coords)\n",
    "\n",
    "    # #combine x/y point groups\n",
    "    coord_all = []\n",
    "    for i in range(len(coord_list)):\n",
    "        coord_group = coord(coord_list[i]) #function to extract and format x/y points\n",
    "        coord_all.append(coord_group) \n",
    "    targets = pd.concat(coord_all)\n",
    "    print('total coordinates (5 per detection):', len(targets))\n",
    "\n",
    "    targets.to_csv(f + n[0] + '.csv', index = None, header=True)\n",
    "\n",
    "    index = pd.DataFrame(envelope.index)\n",
    "    index = index[0] + 1\n",
    "    index.to_csv(f + n[0] + '_index.csv', index = None, header=False)\n",
    "\n",
    "    # #export all centroids\n",
    "    d2 = d2.to_crs({'init': n[1]})\n",
    "    d2.to_file(f + n[0] + '_simrdwn.shp')\n",
    "    d2['geometry'] = d2['geometry'].to_crs(epsg=4326)\n",
    "    d2.to_file(f + n[0] + '_simrdwn.geojson', driver='GeoJSON')\n",
    "\n",
    "    # #export buffered centroids\n",
    "    circles.to_file(f + n[0] + '_simrdwn_pens.shp')\n",
    "    circles['geometry'] = circles['geometry'].to_crs(epsg=4326)\n",
    "    circles.to_file(f + n[0] + '_simrdwn_pens.geojson', driver='GeoJSON')\n",
    "\n",
    "    #export centroids within farmsites\n",
    "    d2_clip.to_file(f + n[0] + '_simrdwn_pts.shp')\n",
    "    d2_clip['geometry'] = d2_clip['geometry'].to_crs(epsg=4326)\n",
    "    d2_clip.to_file(f + n[0] + '_simrdwn_pts.geojson', driver='GeoJSON')\n",
    "\n",
    "    # #export farmsites extents\n",
    "    farmsites2.to_file(f + n[0] + '_simrdwn_farmsites_ext.shp')\n",
    "    farmsites2['geometry'] = farmsites2['geometry'].to_crs(epsg=4326)\n",
    "    farmsites2.to_file(f + n[0] + '_simrdwn_farmsites_ext.geojson', driver='GeoJSON')\n",
    "\n",
    "    # #export farmsites points\n",
    "    farm_pts.to_file(f + n[0] + '_simrdwn_farm_pts.shp')\n",
    "    farm_pts['geometry'] = farm_pts['geometry'].to_crs(epsg=4326)\n",
    "    farm_pts.to_file(f + n[0] + '_simrdwn_farm_pts.geojson', driver='GeoJSON')\n",
    "\n",
    "for i in countries:\n",
    "    simrdwn(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
